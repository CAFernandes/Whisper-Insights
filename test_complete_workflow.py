#!/usr/bin/env python3
"""
Script de teste abrangente para o workflow completo:
1. Upload de arquivos de √°udio de diferentes formatos
2. Transcri√ß√£o com Whisper
3. Gera√ß√£o de insights com Ollama
4. Testes de casos de erro
5. Testes de retry de insights
"""

import requests
import time
import json
import os

# Configura√ß√µes
FLASK_URL = "http://localhost:5001"
TEST_FILES = {
    "valid_audio": [
        ("teste_audio.m4a", "audio/m4a"),
        ("teste_audio.wav", "audio/wav"),
        ("teste_audio.ogg", "audio/ogg"),
        ("teste_audio.kwf", "audio/kwf")
    ],
    "invalid_files": [
        ("invalid_file.txt", "text/plain")
    ]
}

def test_ollama_connection():
    """Testa a conex√£o com o Ollama"""
    print("üîç Verificando conex√£o com Ollama...")
    try:
        response = requests.get(f"{FLASK_URL}/check_ollama")
        data = response.json()
        if data['connected']:
            print(f"‚úÖ Ollama conectado com {data['count']} modelo(s): {', '.join(data['models'])}")
            return data['models']
        else:
            print("‚ùå Ollama n√£o est√° conectado")
            return []
    except Exception as e:
        print(f"‚ùå Erro ao verificar Ollama: {e}")
        return []

def test_whisper_model():
    """Testa se o modelo Whisper est√° carregado"""
    print("üîç Verificando modelo Whisper...")
    try:
        response = requests.get(f"{FLASK_URL}/check_model")
        data = response.json()
        if data['success']:
            print(f"‚úÖ {data['message']}")
            return True
        else:
            print(f"‚ùå Erro no modelo Whisper: {data['message']}")
            return False
    except Exception as e:
        print(f"‚ùå Erro ao verificar modelo Whisper: {e}")
        return False

def upload_file(file_path, content_type):
    """Faz upload de um arquivo espec√≠fico"""
    print(f"\nüì§ Fazendo upload do arquivo: {file_path}")

    if not os.path.exists(file_path):
        print(f"‚ùå Arquivo {file_path} n√£o encontrado")
        return None, f"Arquivo {file_path} n√£o encontrado"

    try:
        with open(file_path, 'rb') as f:
            files = {'file': (os.path.basename(file_path), f, content_type)}
            response = requests.post(f"{FLASK_URL}/upload", files=files)

            if response.status_code == 413:
                return None, "Arquivo muito grande (413 Request Entity Too Large)"

            data = response.json()

            if data['success']:
                print(f"‚úÖ Upload realizado com sucesso. Task ID: {data['task_id']}")
                return data['task_id'], None
            else:
                print(f"‚ùå Erro no upload: {data['message']}")
                return None, data['message']
    except Exception as e:
        print(f"‚ùå Erro durante upload: {e}")
        return None, str(e)

def test_invalid_upload():
    """Testa upload de arquivo inv√°lido"""
    print("\nüö´ Testando upload de arquivo inv√°lido...")

    for file_path, content_type in TEST_FILES["invalid_files"]:
        task_id, error = upload_file(file_path, content_type)
        if task_id is None and "n√£o permitido" in str(error):
            print(f"‚úÖ Erro esperado para {file_path}: {error}")
        else:
            print(f"‚ùå Upload de arquivo inv√°lido n√£o foi rejeitado adequadamente: {file_path}")

def test_no_file_upload():
    """Testa upload sem arquivo"""
    print("\nüö´ Testando upload sem arquivo...")
    try:
        response = requests.post(f"{FLASK_URL}/upload", files={})
        data = response.json()
        if not data['success'] and "Nenhum arquivo" in data['message']:
            print(f"‚úÖ Erro esperado: {data['message']}")
        else:
            print(f"‚ùå Upload sem arquivo n√£o foi rejeitado adequadamente")
    except Exception as e:
        print(f"‚ùå Erro inesperado no teste de upload sem arquivo: {e}")

def monitor_progress(task_id, wait_for_insights=False):
    """Monitora o progresso da transcri√ß√£o e opcionalmente dos insights"""
    print(f"\n‚è≥ Monitorando progresso da task {task_id}...")

    while True:
        try:
            response = requests.get(f"{FLASK_URL}/status/{task_id}")
            data = response.json()
            status = data.get('status', 'unknown')

            if status in ['processing', 'pending_upload']:
                progress = data.get('progress', 'Processando...')
                print(f"üîÑ {progress}")
                time.sleep(3)

            elif status == 'transcription_completed':
                print("‚úÖ Transcri√ß√£o conclu√≠da com sucesso!")
                print(f"\nüìù Transcri√ß√£o:")
                print("-" * 50)
                text = data.get('text', '')
                print(text[:500] + "..." if len(text) > 500 else text)
                print("-" * 50)

                if not wait_for_insights:
                    return data
                else:
                    print("‚è≥ Aguardando gera√ß√£o de insights...")
                    time.sleep(2)

            elif status == 'generating_insights':
                progress = data.get('progress', 'Gerando insights...')
                print(f"üîÑ {progress}")
                time.sleep(3)

            elif status in ['completed_with_insights', 'completed']:
                print("‚úÖ Processamento conclu√≠do com sucesso!")

                if 'text' in data:
                    print(f"\nüìù Transcri√ß√£o:")
                    print("-" * 50)
                    text = data.get('text', '')
                    print(text[:500] + "..." if len(text) > 500 else text)
                    print("-" * 50)

                if 'insights' in data and data['insights']:
                    print(f"\nüß† Insights gerados:")
                    print("-" * 50)
                    insights = data['insights']
                    print(insights[:1000] + "..." if len(insights) > 1000 else insights)
                    print("-" * 50)
                else:
                    print("\n‚ö†Ô∏è Nenhum insight foi gerado")

                return data

            elif status in ['error', 'error_insights']:
                print(f"‚ùå Erro: {data.get('message', 'Erro desconhecido')}")
                return None

            elif status == 'not_found':
                print("‚ùå Task n√£o encontrada")
                return None

            else:
                print(f"‚ö†Ô∏è Status desconhecido: {status}")
                time.sleep(3)

        except Exception as e:
            print(f"‚ùå Erro ao verificar status: {e}")
            return None

def test_retry_insights(task_id, ollama_models):
    """Testa a funcionalidade de retry de insights"""
    print(f"\nüîÑ Testando retry de insights para task {task_id}...")

    if not ollama_models:
        print("‚ö†Ô∏è Nenhum modelo Ollama dispon√≠vel para teste de retry")
        return False

    custom_prompt = "Forne√ßa um resumo executivo muito conciso em 3 pontos principais: {{text}}"

    # Priorizar modelo llama se dispon√≠vel
    selected_model = None
    for model in ollama_models:
        if "llama" in model.lower():
            selected_model = model
            break

    if not selected_model:
        selected_model = ollama_models[0]  # Fallback para o primeiro modelo dispon√≠vel

    try:
        payload = {
            "prompt": custom_prompt,
            "model_name": selected_model
        }

        response = requests.post(f"{FLASK_URL}/retry_insights/{task_id}",
                               json=payload,
                               headers={'Content-Type': 'application/json'})

        data = response.json()

        if data['success']:
            print(f"‚úÖ Insights gerados com sucesso usando modelo {selected_model}")
            print(f"üìù Prompt usado: {custom_prompt[:100]}...")
            if 'insights' in data:
                insights = data['insights']
                print(f"üß† Insights: {insights[:300]}...")
            return True
        else:
            print(f"‚ùå Erro ao gerar insights: {data.get('message', 'Erro desconhecido')}")
            return False

    except Exception as e:
        print(f"‚ùå Erro durante retry de insights: {e}")
        return False

def test_single_file_workflow(file_path, content_type, ollama_models):
    """Testa o workflow completo para um √∫nico arquivo"""
    print(f"\n{'='*60}")
    print(f"üéØ Testando workflow para: {file_path}")
    print(f"{'='*60}")

    # Upload
    task_id, error = upload_file(file_path, content_type)
    if not task_id:
        print(f"‚ùå Falha no upload: {error}")
        return False

    # Monitorar transcri√ß√£o
    result = monitor_progress(task_id, wait_for_insights=False)
    if not result:
        print(f"‚ùå Falha na transcri√ß√£o")
        return False

    # Testar retry de insights se a transcri√ß√£o foi bem-sucedida
    if result.get('status') == 'transcription_completed':
        insights_success = test_retry_insights(task_id, ollama_models)
        if insights_success:
            # Monitorar novamente para ver o resultado final
            final_result = monitor_progress(task_id, wait_for_insights=False)

    print(f"‚úÖ Workflow completo para {file_path}")
    return True

def main():
    """Fun√ß√£o principal do teste abrangente"""
    print("üöÄ Iniciando suite de testes abrangente\n")

    test_results = {
        "whisper_model": False,
        "ollama_connection": False,
        "valid_files": 0,
        "invalid_files": 0,
        "no_file_upload": False,
        "insights_retry": 0
    }

    # 1. Verificar modelo Whisper
    print("üìã FASE 1: Verifica√ß√£o de Depend√™ncias")
    print("-" * 40)
    test_results["whisper_model"] = test_whisper_model()

    # 2. Verificar Ollama
    ollama_models = test_ollama_connection()
    test_results["ollama_connection"] = len(ollama_models) > 0

    if not test_results["whisper_model"]:
        print("\n‚ö†Ô∏è Testes abortados devido a problemas com o modelo Whisper")
        return test_results

    # 3. Testes de upload inv√°lido
    print(f"\nüìã FASE 2: Testes de Valida√ß√£o de Upload")
    print("-" * 40)
    test_invalid_upload()
    test_results["invalid_files"] = len(TEST_FILES["invalid_files"])

    test_no_file_upload()
    test_results["no_file_upload"] = True

    # 4. Testes de workflow completo para arquivos v√°lidos
    print(f"\nüìã FASE 3: Testes de Workflow Completo")
    print("-" * 40)

    successful_workflows = 0
    total_valid_files = len(TEST_FILES["valid_audio"])

    for file_path, content_type in TEST_FILES["valid_audio"]:
        if os.path.exists(file_path):
            success = test_single_file_workflow(file_path, content_type, ollama_models)
            if success:
                successful_workflows += 1
                if ollama_models:  # Se tem modelos Ollama, conta como teste de insights
                    test_results["insights_retry"] += 1
        else:
            print(f"‚ö†Ô∏è Arquivo {file_path} n√£o encontrado, pulando teste...")

    test_results["valid_files"] = successful_workflows

    # 5. Resumo final
    print(f"\n{'='*60}")
    print("üìä RESUMO DOS TESTES")
    print(f"{'='*60}")
    print(f"‚úÖ Modelo Whisper: {'OK' if test_results['whisper_model'] else 'FALHA'}")
    print(f"‚úÖ Conex√£o Ollama: {'OK' if test_results['ollama_connection'] else 'FALHA'} ({len(ollama_models)} modelos)")
    print(f"‚úÖ Arquivos v√°lidos testados: {test_results['valid_files']}/{total_valid_files}")
    print(f"‚úÖ Testes de arquivo inv√°lido: {test_results['invalid_files']}")
    print(f"‚úÖ Teste upload sem arquivo: {'OK' if test_results['no_file_upload'] else 'FALHA'}")
    print(f"‚úÖ Testes de retry insights: {test_results['insights_retry']}")

    # Calcular score geral
    total_tests = 6  # whisper, ollama, valid_files (como um teste), invalid_files, no_file, insights
    passed_tests = (
        (1 if test_results['whisper_model'] else 0) +
        (1 if test_results['ollama_connection'] else 0) +
        (1 if test_results['valid_files'] > 0 else 0) +
        (1 if test_results['invalid_files'] > 0 else 0) +
        (1 if test_results['no_file_upload'] else 0) +
        (1 if test_results['insights_retry'] > 0 else 0)
    )

    success_rate = (passed_tests / total_tests) * 100
    print(f"\nüéØ Taxa de Sucesso Geral: {success_rate:.1f}% ({passed_tests}/{total_tests})")

    if success_rate >= 80:
        print("üéâ Suite de testes APROVADA!")
    elif success_rate >= 60:
        print("‚ö†Ô∏è Suite de testes com AVISOS")
    else:
        print("‚ùå Suite de testes REPROVADA")

    return test_results

if __name__ == "__main__":
    main()
